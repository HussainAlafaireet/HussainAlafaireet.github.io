<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css"
    integrity="sha384-WskhaSGFgHYWDcbwN70/dfYBj47jz9qbsMId/iRN3ewGhXQFZCSftd1LZCfmhktB" crossorigin="anonymous">
  <link rel="stylesheet" type="text/css"
    href="http://mplus-fonts.sourceforge.jp/webfonts/basic_latin/mplus_webfonts.css">
  <link rel="stylesheet" type="text/css"
    href="https://cdnjs.cloudflare.com/ajax/libs/ekko-lightbox/5.3.0/ekko-lightbox.css">
  <!--Lightbox CSS from Ekko Lightbox-->
  <link rel="stylesheet" href="/css/main.css">
  <title>
    Sainy Alafaireet's Portfolio Website
  </title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous">
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.0/umd/popper.min.js"
    integrity="sha384-cs/chFZiN24E4KMATLdqdvsezGxaGsi4hLGOzlXwp5UZB1LY//20VyM2taTB4QvJ" crossorigin="anonymous">
  </script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.0/js/bootstrap.min.js"
    integrity="sha384-uefMccjFJAIv6A+rW+L4AHf99KvxDjWSu1z9VI8SKNVmz4sk7buKt/6v9KI65qnm" crossorigin="anonymous">
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/ekko-lightbox/5.3.0/ekko-lightbox.js">
    //lightbox javascript from Ekko Lightbox
  </script>

  <script>
    $(document).on('click', '[data-toggle="lightbox"]', function (event) {
      event.preventDefault();
      $(this).ekkoLightbox();
    });
  </script>
</head>

<body>
  <main class="container">
    <header role="banner">
      <nav class="navbar navbar-expand-lg navbar-light bg-light" role="navigation">
        <a class="navbar-brand" href="/index.html">
          Sainy Alafaireet
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
          aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
          <ul class="navbar-nav mr-auto">
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown"
                aria-haspopup="true" aria-expanded="false">
                Projects
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                <a class="dropdown-item" href="/Projects/FastEffect.html">AR Product/Design Lead at Fast Effect</a>
                <a class="dropdown-item" href="/Projects/Lifeasagym.html">Life as a Gym AR Concept</a>
                <a class="dropdown-item" href="/Projects/RoomModel.html">The Room Model: A Vision of an XR Browser</a>
                <a class="dropdown-item" href="/Projects/ProceMaze.html">VR Attention Direction within a Procedural
                  Maze</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="/aboutme.html">About Me</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="/resume.html">Resume</a>
            </li>
            <!--<li class="nav-item">
              <a class="nav-link" href="/vrusab.html">VR Usability Assessments</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="/vrdemos">VR/AR Demos</a>
            </li>-->
          </ul>
        </div>
      </nav>
    </header>
    <div class="container" role="main">
      <div class="row">
        <div class="col-12">

          <h1>Life as AR Glasses - Exercise Guidance for a Busy Lifestyle</h1>

          <h2>Project Overview - Smart Glasses that encourage exercise in a busy life through thoughtful time management
            and
            a
            deep understanding of one’s lifestyle </h2>
          <p>
            I worked with four other UX professionals to prototype <strong>Life As a Gym (LG)</strong>, an innovative
            method
            to encourage University students to add more exercise
            to their already-filled lives. LG glasses are enhanced with augmented reality that scans a user’s
            environment
            for ad-hoc exercise opportunities, such as stairs to climb, places to jog, etc. LG glasses then inform their
            users to these exercise opportunities and guides the users to them. LG manages these ad-hoc opportunities to
            the
            users’ schedule, giving them the confidence to exercise, knowing that they will still make it to their next
            stop
            on time.
          </p>
          <br>
          <h3>Key LG Features</h3>
          <br>
          <div class="row">
            <div class="col-lg-4 col-sm-12 ">
              <h4>AR Cameras</h4>
              <p>Actively scan the environment and allows the AR interaction to occur.</p>
            </div>
            <div class="col-lg-4 col-sm-12">
              <h4>Time & Schedule Integration</h4>
              LG integrates with your electronic calendar and knows when you need to be somewhere.
            </div>
            <div class="col-lg-4 col-sm-12">
              <h4>Navigation</h4>
              LG acts as a personal navigation device with turn-by-turn guidance through any
              activities
              you perform.
            </div>
          </div>
          <div class="row">
            <div class="col-lg-4 col-sm-12">
              <h4>Voice Assistance & Input </h4>
              Users can hear LG glasses speak, and they can interact with the glasses using voice.
            </div>
            <div class="col-lg-4 col-sm-12">
              <h4>Exercise Moments</h4>
              LG smartly manages your time, and recommends exercises that fit into your schedule.
            </div>
            <div class="col-lg-4 col-sm-12">
              <h4>Companion App</h4>
              LG comes with a companion application that gives users more in-depth information
              about
              their daily exercise.
            </div>
          </div>





          <br>


          </p>

          <h2>Demonstration Video </h2>
          <div class="embed-responsive embed-responsive-16by9">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/D9FwlmPfrUI" frameborder="0"
              allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          </div>

          <h2>My Contributions - Timeslines, User Research, and Prototype Coding </h2>
          <p>
            My primary contributions to this project were as follows:
            <ol>
              <li>Managed the overall group activity as team lead, balancing workload and defining timetables.</li>
              <li>Acted as one of the primary designers and facilitators of the user research used.</li>
              <li>Developed the live Wizard-of-Oz demo of LG utilizing the AFrame AR/VR web framework, along with the
                AR.js
                AR library.</li>
            </ol>
          </p>

          <h2>Problem to be Solved - How do we get University students to actually exercise regularly? </h2>

          Our motivation for creating LG was a combination of several different factors. Prior to deciding to go with
          Life
          as
          a Gym glasses as our concept, we conducted background research in various topics; one of the statistics we
          found
          was approximately 40-50% of all college students do not get the recommended amount of physical activity, that
          is,
          they do not do enough physical activity in a given day to constitute exercise from a medical perspective. This
          statistic was a driving force throughout our project and stuck with us as one of our key motivations
          throughout
          the course.
          <p></p>


          <h2>Initial Ideation and Scoping - Health...And Memory</h2>

          <p>
            Our original goal for this design challenge was extremely high-level. We knew we wanted to prototype a smart
            device, and we wanted to
            have that device support University student health, as that was a challenge we had all faced at one time or
            another. To get our ideas flowing, and
            to point some firmer bounds around the scope of the project, we engaged in an initial ideation session. As
            we
            developed our concepts, we noticed that,
            in addition to the Unversity health scoping, we were also coming up with functionality that revolved around
            memory and the use of reminders
            to manage one's activity.
          </p>

          <div class="row">
            <div class="col-2"></div>
            <div class="col-8">
              <figure href="/assets/LG/ms1_day_1_sketches.jpeg" class="figure" data-toggle="lightbox"
                data-footer="Our brainstorming on better scoping out our problems space.">
                <img src="/assets/LG/ms1_day_1_sketches.jpeg" alt="" class="img-fluid">
                <figcaption class="figure-caption text-center">Our brainstorming on better scoping out our problems
                  space.
                </figcaption>
              </figure>
            </div>
            <div class="col-2"></div>
          </div>

          <div class="row">
            <div class="col-2"></div>
            <div class="col-8">
              <figure href="/assets/LG/ms1_day2_sketches.JPG" class="figure" data-toggle="lightbox"
                data-footer="Concept sketches for the area of health reminders">
                <img src="/assets/LG/ms1_day2_sketches.JPG" alt="" class="img-fluid">
                <figcaption class="figure-caption text-center">Concept sketches for the area of "health reminders"
                </figcaption>
              </figure>
            </div>
            <div class="col-2"></div>
          </div>

          <p>
            Based on the two brainstorming pillars of "University student health" and "reminders" we developed several
            more
            fleshed-out concepts. My contribution to the concept pool
            was a smart gym concept:
          </p>

          <h3>Smart Gym</h3>

          <p>This concept is a gym with health-monitoring capabilities to maximize the time and effort of off-campus
            students who will have a limited window to both learn and use the gym. The concept included the following
            primary features:

            <ul>
              <li><strong>Easy Login: </strong>A login board that accepts the student’s University ID. Tied to that ID
                is
                the student’s gym history. When the student first arrives at the gym and swipes their card, the login
                board
                will access the student’s history and generates a workout plan for the session.</li>
              <li><strong>Automatic User-based Settings For Workout Machine: </strong>Each individual workout machine
                would
                have it’s own data connection, and will be able to access the student’s history. Each machine would set
                up
                it’s own weights, resistances, training times, and other settings to create a “just-right” challenge for
                the
                student.</li>
              <li><strong>Workout Machine Sensors</strong>The workout machines would have sensors built into them to
                measure
                the physical state of the student as the machine is being used. The workout could then be adjusted on
                the
                fly
                to match the physical condition of the student. The machine itself could tell when the student needed to
                rest,
                and could suggest that the student hydrate when needed, unlocking an on-site drink dispenser to provide
                hydration.</li>
            </ul>

          </p>

          <div class="row">
            <div class="col-2"></div>
            <div class="col-8">
              <figure href="/assets/LG/ms1_smart_gym.jpg" class="figure" data-toggle="lightbox"
                data-footer="Concept sketches for the area of health reminders">
                <img src="/assets/LG/ms1_smart_gym.jpg" alt="" class="img-fluid">
                <figcaption class="figure-caption text-center">Smart Gymn concept sketch</figcaption>
              </figure>
            </div>
            <div class="col-2"></div>
          </div>



          <h2>User Research Round 1 - A Micro Method, and a Macro One </h2>
          <p>Now that we had some scoping and preliminary concepting done, it was time to conduct user research to
            understand
            real user needs, and to see if our initial brainstorming was on track. We began by narrowing down the
            research
            questions we wanted to answer. We ended up with
            5 primary research questions:
            <br>

            <ol>
              <li>What motivates the target audience to exercise?</li>
              <li>What factors prevent target from exercising?</li>
              <li>How does target audience learn how to exercise?</li>
              <li>What kind of exercises does our target audience actually do?</li>
              <li>What resources does our target audience use to enable their exercise?</li>
            </ol>

            After defining our research questions we then narrowed down what methods we would be to answer them. For
            Research Question 1, 2, and 4
            we chose to go with a diary study. We felt that we would get more specific and more honest answers for these
            questions if we asked them "in the moment", as participants were completing their exercises. For research
            questions 3, 4, and 5, we chose to go with a survey, as these were questions tightly-tied to scale, with the
            most important part of the questions being "how much", rather than "what"? Research question 4 double-dipped
            into
            both methods because
            it easily fit into both methods
            and getting validation from both methods would strengthen the overall results.
          </p>

          <h3>Diary Study Setup</h3>
          <p>For our diary study we chose to reach out to our social networks and recruit 3 people to take our diary
            study
            for 6 days. We informed the participants that there will be a monetary gift card awarded for completion of
            the
            diary study. The goal of the diary study was to gain insight into the day-to-day exercise motivations and
            barriers that our participants faced (Research Questions 1 and 2). In particular, we were hoping to gain
            insight
            into novel barrier situations that would not have occurred to us to include in our survey. Results from the
            diary study were analyzed with a light affinity wall analysis to pull out the key concepts, and then were
            organized against the research questions, pairing them with the corresponding survey results for each
            question.
          </p>

          <h3>Survey Setup</h3>
          <p>As previously mentioned, for the survey we focused on answering research questions 3, 4 and 5. We wrote a
            simple survey with about 16 questions. After creating the survey we distributed the survey on multiple
            channels.
            The first channel we sent out our survey is through the UMSI list serve. After this, we distributed it on 5
            college forums on the popular social media website, Reddit. The 5 college schools were: University of
            California
            at Berkeley, University of Illinois at Urbana-Champaign, University of Central Florida, Georgia Institute of
            Technology, and University of Michigan. During our recruiting, we indicated that taking the survey will
            enroll
            them into a raffle if they wished to leave their email at the end. After 5 days we cut off data collection
            with
            176 responses. For the preliminary demographic results of the survey, 142 responses marked that they
            exercised
            in the past 2 weeks of taking the survey. About 49% were female, 47% were male and ~3% didn’t disclose or
            were
            nonbinary.</p>

          <h3>Round 1 Research Results</h3>

          <p> We had a rough mapping of survey and diary questions relative to their corresponding research question. To
            analysis our results, we first tabulated our quantitative
            data and performed light affinity wall analysis to categorize our qualitative data from the diary. We then
            formalized this mapping during our post-survey analysis, directly relating the results of each study
            question
            back
            to a core research question.

            <div class="row">
              <div class="col-2"></div>
              <div class="col-8">
                <figure href="/assets/LG/ms2_data_whiteboard.jpg" class="figure" data-toggle="lightbox"
                  data-footer="Extended mapping of survey results to research questions">
                  <img src="/assets/LG/ms2_data_whiteboard.jpg" alt="" class="img-fluid">
                  <figcaption class="figure-caption text-center">Extended mapping of survey results to research
                    questions
                  </figcaption>
                </figure>
              </div>
              <div class="col-2"></div>
            </div>


            By having this mapping viewable on a whiteboard as a whole, we were also able to
            see
            patterns between the different activities and questions, which lead us to three core insights:

            <ol>
              <li><strong>Lack of time/time management: </strong>This is by far our most important finding. We found
                that
                time was both the primary concern and the primary metric our target used relative to exercise.
                This manifested itself in 2 main ways. Firstly, University students tended to measure their exercise out
                in
                terms of time rather than the direct amount of exercise (i.e. students would tend
                to measure running goals in terms of "running for an hour" rather than "running a mile". Secondly, lack
                of
                time was cited as the primary barrier students faced in actually completing exercise. </li>
              <li><strong>Endurance and Strength exercises were the most prominent forms of exercise.: </strong>Our
                survey
                results in particular rated running and biking as the most common exercises our target audiences
                performed.
                Strength (weight training) was rated as the next highest category, with floor training being the next,
                and
                still somewhat popular, category. We would
                therefore need to make sure our solution covered these forms of exercise extensively.</li>
              <li><strong>Users often need guidance when doing exercises to learn how to do them correctly: </strong>A
                significant amount of discussion
                in both the survey and the diary was the need for education on how to do an exercise correctly. This was
                particularly apparent in the use of strength exercise, where proper
                form is particularly important, both for effectiveness, and to prevent injury.</li>
            </ol>


            <h2>Design Concept Round 2 - Time Manangment + Exercise Guidance = Product Win</h2>


            <p>Based on our new primary design insights, we generated 3 new design concepts. The concept I devised
              (which is
              the concept we ended up going with)
              was called "Life as a Gym".
            </p>

            <h3>Revised Design Concept: Smart Exercise Mat</h3>

            <div class="row">
              <div class="col-2"></div>
              <div class="col-8">
                <figure href="/assets/LG/ms2_smart_exercise_mat.JPG" class="figure" data-toggle="lightbox"
                  data-footer="Smart Exercise Mat Design Concept">
                  <img src="/assets/LG/ms2_smart_exercise_mat.JPG" alt="" class="img-fluid">
                  <figcaption class="figure-caption text-center">Smart Exercise Mat Design Concept
                  </figcaption>
                </figure>
              </div>
              <div class="col-2"></div>
            </div>

            <p>Our first project concept was the Smart Exercise Mat. This concept was primarily designed to help people
              learn proper
              form when exercising, and also help them exercise in more convenient ways than having to go to a dedicated
              space for exercising (e.g. a gym). The mat featured a built-in display that will show users how to do
              specific
              exercises with the correct form, and also featured LED lights that will show users where to place certain
              body
              parts for certain exercises. As a more concret example, for a push-up, the display would show a user an
              instructional video on how to do a push-up, and then the mat
              itself would light up to show where the users arms and feet were supposed to go. This project concept was
              created to
              address users need for guidance during exercise, and also to support the fact that floor exercises were
              among
              the most popular during our survey results.</p>

            <h3>Revised Design Concept: Smart Gesture Guide</h3>

            <div class="row">
              <div class="col-2"></div>
              <div class="col-8">
                <figure href="/assets/LG/ms2_smart_gesture_guider.JPG" class="figure" data-toggle="lightbox"
                  data-footer="Smart Gesture Guide Concept"">
              <img src="/assets/LG/ms2_smart_gesture_guider.JPG" alt="" class="img-fluid">
                  <figcaption class="figure-caption text-center">Smart Gesture Guide Concept"
                  </figcaption>
                </figure>
              </div>
              <div class="col-2"></div>
            </div>

            <p>The next Revised Design Concept project was called the “Smart Gesture Guider,” which was designed to
              help
              our users learn
              proper form during their workouts. The guider was a portable camera unit equipped with sensors that
              observes a
              user during
              their workouts and tracks them to see if they are doing certain exercises correctly. When the guider
              detected
              incorrect form, it will start playing a video that shows a user how to do the correct form of the exercise
              they are attempting. For example, if a user is trying to do Russian Twists in front of the camera and has
              incorrect form while doing them, the camera will notice this, and then immediately find a video from a
              trusted
              source that shows how to perform them correctly without the user needing to interact with the system at
              all.
              The goal of this concept was to address users need for proper guidance while completing exercises, while
              also
              keeping them totally immersed in the exercise experience.</p>

            <h3>Revised Design Concept: Life As A Gym</h3>

            <div class="row">
              <div class="col-2"></div>
              <div class="col-8">
                <figure href="/assets/LG/ms2_life_as_gym.JPG" class="figure" data-toggle="lightbox"
                  data-footer="Life as a Gym AR Glasses concept">
                  <img src="/assets/LG/ms2_life_as_gym.JPG" alt="" class="img-fluid">
                  <figcaption class="figure-caption text-center">Life as a Gym AR Glasses concept
                  </figcaption>
                </figure>
              </div>
              <div class="col-2"></div>
            </div>

            <p>Our final project concept (which I was the originator of) was called “Life as a Gym” - a solution
              designed to
              help users exercise anywhere and
              anytime. This concept involves creating Augmented Reality glasses that have the ability to scan the
              everyday
              world as a user goes
              through it and recognize certain objects or structures which can provide good opportunities for quick, on
              the
              fly workouts. For example, if a user was wearing the glasses and they come across a staircase, the glasses
              would
              recommend ways the user can interact with it to create a small workout in that immediate space.
              Additionally,
              the glasses would be integrated into a user’s schedule, daily routines, and would also track a users
              habits
              while they were wearing them. Using this integration, the glasses would have the ability to offer
              suggestions
              on how
              to fit exercises into a users schedule given the context of their day. This project concept was created to
              address the fact that our target audience often suffers from having little to no time to exercise.

              We ended picking the Life as a Gym concept for two main reasons. Firstly, we felt that out of our three
              solutions, the glasses best addressed
              the primary consideration of time management. Secondly, we were the most interested in the concept from a
              creative standpoint, due to the futuristic nature
              of using AR technology to enable physical exercise.

              <h2>User Enactments - To LG or not to LG?</h2>

              <p>Thanks to our initial user studies, we had a solid understanding of the primary factors affecting how
                our
                target audience approaches physical exercise,
                and "Life as a Gym" (LG) - a design concept that directly addressed those factors. However, we knew we
                needed more user input in order to understand the exact set of features LG should have. To determine
                this,
                we we decided to conduct User Enactments. The goal of User Enactments is to have users go through and
                experience the concept the designers are trying to bring to life; by “acting” out various scenarios in
                which
                the
                product or service will be used. User Enactments were extremely helpful for our specific concept because
                it
                revolves around a technology that was still fairly new, and that most people have yet to interact with.

                When developing the scenarios for our user enactments we had two distinct goals in mind for each
                scenario.
                The primary goal was to test out a unique feature that our glasses would have. For example, one scenario
                is
                focused on testing navigation, while another scenario would test out more of the exercise component. The
                second goal of each enactment was to learn more about users perceptions of augmented reality. We decided
                to
                add this second goal because AR is a fairly new technology that most people haven't used or experienced
                yet.
                We wanted to ensure that the system we are developing meets their expectations about what AR is, but
                also
                creates a system that doesn’t cross any personal lines or boundaries.
              </p>

              <h3>User Enactment Study Design</h3>
              <h4>Setting</h4>
              <p>To help make the user enactments more realistic, we utilized large rooms and hallways within two
                University
                classroom buildings. to conduct our
                scenarios. These areas were chosen for two reasons: first, we needed a large enough space so users could
                walk around
                as if they were wearing the glasses, and second, it would be difficult to test outdoors because of
                different
                lighting and weather conditions.</p>

              <h4>Moderation</h4>
              <p>During our user scenarios, we provided several different moderation styles to try and test how users
                would
                expect LG glasses to work. In scenarios 1 and 4 for example, the LG glasses would speak directly to our
                users, while scenarios 2 and 3 made users read screens manually with no voice assistance. In addition to
                receiving feedback from our scenarios, this helped us learn how participants would like LG to interact
                with
                them, and how the participants can interact with LG.
                Additionally, before each scenario began, users were given a description of the LG system, which is as
                follows: <br>

                <i>"Life as a Gym, or LG for short is an augmented reality system that integrates into your daily
                  routine
                  and
                  schedule and helps you find exercise opportunities as you go about your everyday life. There are three
                  different ways that you can interact with the system, you can use voice commands, gesture controls, or
                  there
                  are also two buttons located on each side of the glasses which allow you to interact with the
                  system."</i>
                <br><br>

                Users were then given the chance to ask questions to clear up any misunderstandings or confusion
                regarding
                the system.</p>

              <h4>Stationary vs. Mobile</h4>
              <p>We had two distinct ways of running our scenario’s separate of moderation: <br>

                <ol>
                  <li>The first way we conducted our scenarios was stationary, for where we had users stand or sit in
                    one
                    spot while they went through our scenarios. We used stationary moderation for those scenarios that
                    involved set-up or planning operations, rather than the core experience of using LG in a route.
                    Several
                    of the scenarios we enacted such as scenario 1, 4, and 5 did not require users to move around.</li>
                  <li>The second way we conducted our scenarios was having participants walk around, while the moderator
                    of
                    the scenario would be walking in front of them and showing them screens at certain points throughout
                    the
                    scenario. We chose to do this type of moderation because we wanted to get a slightly more realistic
                    view
                    of how the glasses would work in a real-world setting. Additionally, having users walk around
                    provided a
                    lot of feedback about how they felt about doing certain tasks while having to move.</li>
                </ol>
              </p>

              <h4>Participants</h4>
              <p>In total, we ended up testing LG with 9 unique participants. However, not all participants were able to
                interact with every part of the system. When we performed our original scenario testing, we only had
                four
                distinct scenarios. Initially, we ran through these scenarios with six unique users. Then, we decided to
                add
                an additional three scenarios based on feedback and ideas that we received about the system. These three
                scenarios were tested by, an extra three unique participants, and also two returning participants.</p>

              <h3>User Enactment Scenarios</h3>

              <h4>User Scenario 1 - Travel in a New City</h4>
              <h5>Scenario 1 Walkthrough</h5>
              <p>In scenario 1, we had users work through planning out their trip in Cairo, Egypt. They were shown
                screens
                that allowed them to pick various locations to travel to, and also included a map screen that let users
                see
                their routes and stops along the way. Users were first asked to put on our prop glasses, receiving a
                welcome
                back screen after they did so. Next, users received a dialog screen asking them to either “Plan a Trip”
                or
                “Start a Trip”. Our intent was for the user to use the “Plan a Trip” option, but we accepted either
                input
                here. Next, users received a loading screen indicated that LG was automatically building a schedule for
                them
                without their direct input. Finally, LG showed the user the final scheduled route, so that the users
                would
                know what places they would be visiting.</p>

              <div class="row">
                <div class="col-2"></div>
                <div class="col-8">
                  <figure href="/assets/LG/ms3_ue2.JPG" class="figure" data-toggle="lightbox"
                    data-footer="Scenario 1 Participant planning their trip to Cairo">
                    <img src="/assets/LG/ms3_ue2.JPG" alt="" class="img-fluid">
                    <figcaption class="figure-caption text-center">Scenario 1 Participant planning their trip to Cairo
                    </figcaption>
                  </figure>
                </div>
                <div class="col-2"></div>
              </div>


              <h5> Scenario 1 Goal</h5>
              <p>The primary goal of scenario 1 was to test out how users felt about using our Life as a Gym glasses to
                schedule and plan out their day. We wanted to see if this felt like a natural interaction, or if they
                were
                more inclined to use something else for planning. Secondly, if users did want to use the glasses for
                such a
                function, it allowed us to learn what steps should be involved with the process and how everything
                should
                function. Finally, this was one of the scenarios that tested high level of automation. We wanted to see
                if
                users were comfortable with LG having complete control of the experience, and if not, what the
                boundaries
                for automation were.</p>

              <h4>User Scenario 2 - Typical Day on Campus</h4>
              <h5>Scenario 2 Walkthrough</h5>
              <p>In scenario 2, we had users assume the role of a college student that is on their way class. They put
                on
                their LG glasses and are presented with a screen that says “Would you like to play a route to class?” If
                users select yes, they are then presented with 2 different route screens, when they select which route
                they
                would like to take, the scenario changes into navigation mode where the user is shown a piece of paper
                with
                an arrow on it, guiding them where to walk. After walking for a little bit, the user is shown a screen
                that
                says, “Exercise opportunity found: Stairs” along with a picture of the activity. If the user accepts the
                activity, they are shown the navigation arrow again until they get to the area with the stairs. Once
                they
                arrive at the stairs, they are shown a screen that tells them how to do the exercise, in this case,
                stretch,
                walk up the stairs, and repeat 3 times. Once the user finishes their exercise, the system shows them a
                summary of what they did in terms of calories burned and duration, and then switched back to the
                navigation
                arrow, to guide them on the rest of their route.</p>
              <h5> Scenario 2 Goal</h5>
              <p>The primary goal of the second scenario was to test out how the glasses will work when a user is
                walking
                around on a route that they know well. This scenario also helped us get feedback about how users would
                like
                to be alerted about exercise opportunities, how they would like navigation to work, and what kinds of
                information needs to be displayed to users while wearing the glasses.</p>

              <h4>User Scenario 3 - Campus Scenario with Multiple Routes</h4>
              <h5>Scenario 3 Walkthrough</h5>
              <p>In scenario 3, we asked users to pretend that they got out of class earlier than their schedule said,
                and
                also that they had a meeting across campus in 20 minutes. The scenario begins with a reminder that LG
                glasses have detected you are no longer in class, and that they have detected several routes a user can
                take
                to get to their meeting. The user is then presented 2 different routes for them to take, one that is
                shorter
                and costs them less time, and one that is longer and costs them more time. Once users select their
                route,
                they are shown a navigation screen that shows them a street view of where they are, and where they
                should
                make upcoming turns. After walking for a bit the user is then shown an exercise alert that has a graphic
                along with the amount of time it will take to complete the exercise. If the user says yes, then they
                will go
                off until they reach the exercise, and then after their exercise is complete they will be directed back
                on
                route to their destination and shown a summary screen of the activities they did. If they say no, they
                will
                switch back to the street view of guidance until they reach their destination, where once again they
                will
                receive a summary screen of what they did.</p>
              <h5> Scenario 3 Goal</h5>
              <p>For user scenario 3, we decided to focus on how our concept would work if a user was not adhering to
                the
                schedule that the system knew. This scenario gave us a chance to see how user’s would feel about the
                system
                automatically knowing that they were no longer in class, and how they would respond to the system with
                that
                in mind. Additionally, scenario 2 focused on the concept of time; users needed to pay attention to both
                how
                long the routes would take, and how long the exercise in the middle would take in order to make it to
                their
                meeting in time.</p>


              <h4>User Scenario 4 - LG at a Shopping Mall with a Friend</h4>
              <h5>Scenario 4 Walkthrough</h5>
              <p>In scenario 4, we asked users to pretend they were going to the mall with a friend of theirs. LG is
                already
                aware of their schedule and knows exactly who they are going to the mall with. When the user gets to the
                mall, they are alerted with a screen saying there is an exercise opportunity. They then get to choose if
                they want to alert their friend about the exercise opportunity. If the user selects no, then the glasses
                prompt the user to continue the exercise by themselves, however, if the user selects yes, then the
                friend is
                automatically sent a text message which asks them if they would like to join the main user in doing an
                exercise; the alert also tells the friend exactly what the exercise is. If the friend selects yes, the
                users
                then meet up and do the exercise, if the friend selects no, the user is then prompted to “double-check”
                with
                the friend manually; once the user checks with the friend they can let the system know their decision.
                If
                that decision is a no, then the system asks if the user wants to continue by themselves, if the user
                selects
                yes, then the system guides both users to the exercise.</p>
              <h5> Scenario 4 Goal</h5>
              <p>The primary goal of scenario 4 was to learn about how users would feel about using the LG glasses in a
                social setting and also explore where the boundaries were for users that the glasses needed to respect.
                For
                example, would they enjoy getting notifications from LG when they are out with their friends?
                Additionally,
                this scenario let us test out how and if the LG glasses should have any interaction with the primary
                user’s
                friends, and how the user would feel about the system sending automatic notifications to the friend
                without
                the user having control over them.</p>

              <div class="row">
                <div class="col-2"></div>
                <div class="col-8">
                  <figure href="/assets/LG/ms3_ue1.JPG" class="figure" data-toggle="lightbox"
                    data-footer="Scenario 4 Participant wearing LG at a mall with friends">
                    <img src="/assets/LG/ms3_ue1.JPG" alt="" class="img-fluid">
                    <figcaption class="figure-caption text-center">Scenario 4 Participant wearing LG at a mall with
                      friends
                    </figcaption>
                  </figure>
                </div>
                <div class="col-2"></div>
              </div>

              <h4>User Scenario 5 - First Time Use</h4>
              <h5>Scenario 5 Walkthrough</h5>
              <p>In this scenario, users are taken through the onboarding mobile application that will be a part of the
                LG
                glasses. When they start the scenario they are asked to sign in or make a new profile, as this is the
                first
                time they’ve used the system, they make a profile. Users are then presented with a screen asking for
                physical information about them: name, age, weight, and height. When users click to go to the next
                screen,
                they are asked to select several exercises that they would like LG to “find” (In this case “find” means
                what
                exercises it scans for and shows the user in their environment) for them. After selecting the exercises,
                the
                user is then asked to sign into the email associated with their calendar. Once signed in, they get taken
                to
                a screen which reads, “Select which activities you would like LG to automatically monitor and update.”
                The
                user then selects which activities from a list of: route information, exercises, favorite exercises, and
                pace/speed. After this screen the user is shown the profile completed screen. Once the user clicks out
                of
                that screen, they are instructed to put on the glasses and are shown the permissions screen. The
                permissions
                screen asks for camera, microphone, contacts, and location permissions.</p>
              <h5> Scenario 5 Goal</h5>
              <p>The primary goal of scenario 5 is to learn more about what information users are comfortable with LG
                knowing as they use the glasses. We felt it was crucial that we explore how users felt about privacy and
                automation during our user enactment. This scenario provided meaningful context to have that
                conversation
                with users. Additionally, this scenario helped us explore how much information users need to make
                choices
                when setting up the tracking and monitoring aspects of the system. For example, do they need to know why
                LG
                is asking for permission to access their calendar?</p>

              <h4>User Scenario 6 - Campus with Multi-Routes Redux</h4>
              <h5>Scenario 6 Walkthrough</h5>
              <p>Users walked through the same process that was listed for scenario 3, with the only addition being a
                screen
                shown at the end which informed the user that their route information and pace were updated, along with
                the
                stairs being added as a favorite exercise.</p>
              <h5> Scenario 6 Goal</h5>
              <p>The primary goal of scenario 6 was to learn about the privacy and monitoring aspect of LG glasses
                during
                the actual route experience. Specifically, we wanted to see how users felt when presented with a message
                that the system had been automatically monitoring them during one of their walks to a destination. Would
                they be okay with the monitoring? Does it creep them out? Questions such as these were what we wanted to
                answer with scenario 6.</p>

              <h4>User Scenario 7 - Travel in a New City - User Going off the Route</h4>
              <h5>Scenario 7 Walkthrough</h5>
              <p>In this scenario, we asked users to pretend they are visiting another country, in this case Cairo,
                Egypt.
                Users are told at the beginning to start heading off in a new direction that was not part of their
                originally planned route. As user’s start heading in that direction they receive a prompt which alerts
                them
                that the system has detected they are no longer on their route, and also asks them if there is somewhere
                else they would like to go. If a user says yes they are shown a screen asking them where to go; when
                they
                input their destination, another screen comes up asking them if they would like to add that specific
                screen
                to their schedule. If the user says yes, then they are shown a map screen with their route information
                updated. If the user chooses no, then the navigation ends immediately, and alerts the user that they can
                resume their old route at any time they wish.</p>
              <h5> Scenario 7 Goal</h5>
              <p>The primary goal of scenario 7 is to learn about how the system reacts when a user chooses to ignore
                its
                command, and also learn how the user wants the system to handle that situation. Additionally, this
                scenario
                helped us learn about and engage in a dialogue with our users about how the personal navigation aspect
                of
                our system should work.</p>

              <h3>User Enactment Iteration</h3>
              <p>As we worked with different users through our various scenarios, we encountered several issues with
                several
                of the scenarios and UI screens which needed to be addressed for future users. In particular, after our
                initial test with our first user, each member of our team made significant changes to both our scenarios
                and
                the screens presented. In general, there were a lot of issues with map based screens not being accurate
                enough for users to make decisions, and we also received feedback about too much text being on our
                initials
                set of screens. We reworked out map screens to be more like Google Maps, and we replaced text with
                graphics
                where possible. <br><br>


                <div class="row">
                  <div class="col-2"></div>
                  <div class="col-8">
                    <figure href="/assets/LG/ms3_iter1.png" class="figure" data-toggle="lightbox" data-footer="Before/After Change of map screen in the
                      pre-planning process. Switch visualization of scheduled trip stops from a list to a Google-Maps
                      style screen.">
                      <img src="/assets/LG/ms3_iter1.png" alt="" class="img-fluid">
                      <figcaption class="figure-caption text-center">Before/After Change of map screen in the
                        pre-planning process. Switch visualization of scheduled trip stops from a list to a Google-Maps
                        style screen.
                      </figcaption>
                    </figure>
                  </div>
                  <div class="col-2"></div>
                </div>

                <div class="row">
                  <div class="col-6">
                    <figure href="/assets/LG/ms3_iter2.jpg" class="figure" data-toggle="lightbox"
                      data-footer="Before-iteration screens had too much text, and were therefore hard for users to parse while walking.">
                      <img src="/assets/LG/ms3_iter2.jpg" alt="" class="img-fluid">
                      <figcaption class="figure-caption text-center">Before-iteration screens had too much text, and
                        were therefore hard for users to parse while walking.
                      </figcaption>
                    </figure>
                  </div>
                  <div class="col-6">
                    <figure href="/assets/LG/ms3_iter3.jpg" class="figure" data-toggle="lightbox"
                      data-footer="After-iteration screens struck a better balance between text and imagery.">
                      <img src="/assets/LG/ms3_iter3.jpg" alt="" class="img-fluid">
                      <figcaption class="figure-caption text-center">After-iteration screens struck a better balance
                        between text and imagery.
                      </figcaption>
                    </figure>
                  </div>
                </div>



                In addition, we found that Scenario 1 and Scenario 4 were not returning as useful of data in general as
                we
                had hoped. Scenario 1 did not utilize features unique to our app, and the social situation in Scenario 3
                was
                universally rejected by our initial batch of test users. To compensate, we added 3 new scenarios for a
                second batch of test users. <br><br>

                Additionally, when we added our extra three scenarios, we purposefully designed the screens to more
                closely
                relate to some of the issues that we had with our original four scenarios and also to address the
                feedback
                we had gotten from our original 6 users. Below are some of the changes that we made to various screens
                and
                scenarios:
              </p>

              <h3>Debriefing Sessions:</h3>
              <p>
                After each enactment concluded, we sat participants down and engaged in an in-depth debrief session to
                learn
                more about their experience using the LG glasses. Our debrief sessions generally lasted between 20-30
                minutes, however some sessions approached an hour spent just on the debrief. Each debriefing session
                went a
                little different, because each participant had their own unique view and comments about the concept.
                <br><br>

                Additionally, we also had smaller, scenario specific debriefing sessions after a user would complete a
                scenario. For example, if we just finished running through scenario 1, we would ask our participants
                specific questions about their experience with that scenario. These small debriefs also aimed at
                clearing up
                any issues that were experienced during a scenario. For example, if a user had a tough time interpreting
                a
                screen we would address it after the scenario so it was still fresh in their mind. <br><br>

                Our debriefing question list was as follows:
                <ol>
                  <li>Overall, what did you think about the LG glasses?</li>
                  <li>What are your thoughts on AR?</li>
                  <li>What did you think about walking and reading?</li>
                  <li>What kind of indicators would you like LG to use?</li>
                  <li> What is your preferred way of interacting with LG?</li>
                  <li>What information do you expect to see/hear about exercises?</li>
                  <li>What kind of visualizations do you expect to see with a product of this type?</li>
                  <li>What aspects of the experience didn’t line up with your expectations?</li>
                  <li>Do you want LG to look at their camera?</li>
                  <li>How much do you want LG to tell you what to do and where to go?</li>
                  <li>How do you want LG to react when you go off route?</li>
                  <li>How do you prefer information being presented on LG?</li>
                  <li>What sorts of information should LG know about you?</li>
                  <li>How should LG react in social situations?</li>
                  <li>What do you think about LG potentially tracking and monitoring you?</li>
                </ol>
              </p>

              <h3>User Study Analysis</h3>
              <p>In order to analyze all of the data we collected, we created affinity notes from each user and for each
                scenario, and then constructed an affinity wall to categorize them. <a
                  href="https://miro.com/app/board/o9J_kysbjJc=/">The full affinity wall can be viewed here</a>, but the
                following are the primary points taken from the analysis:</p>

              <h4>Users want LG to be a solo experience</h4>
              <p>We had initially hypothesized that users would enjoy exercising with other people and doing certain
                exercises in the public. We learned in our user enactments that this wasn’t the case at all. In fact,
                many
                people were uncomfortable dragging their friends into doing exercises together in public. Our users
                wanted
                to prioritize their friends, and not the exercise experience. We also learned that spontaneous push-ups
                or
                situps were socially awkward for many people. We need to restrain the exercise types LG will suggest to
                those that will not be really obvious in public. We learned these results from scenario 2 and 4.</p>

              <h4>Users Expect LG to remain within its purpose of exercise time management and restrict its data
                collection/use otherwise</h4>
              <p>Although we valued user's privacy tremendously, it was fascinating to learn that our concept did
                generate
                privacy concerns, and didn't provide clear explanations of data collection throughout the process. Many
                users wanted our concept to primarily focus on fitness related information. They did not want LG deep
                diving
                calendar details or tracking personal locations, such as homes. They also expressed interest in
                minimization
                of the data that LG collects. Ultimately, they wanted to understand the intricacies of how their data is
                being used. On additional caveat to these findings was the use of voice. Some users expressed
                dissatisfaction with using voice all the time. We learned these results from scenario 3, 4, 5, 6 and 7.
              </p>

              <h4>Core LG time management paradigm is time-optimized routes relative to a calendar schedule</h4>
              <p>One of our hypothesis for our concept was that LG would be responsible for handling user's time and
                suggesting exercises and routes that fit their schedule. This hypothesis was further refined in our UEs
                because many users expressed interest in improving how LG routes them based on how much time they have
                available until their next appointment/meeting. They had also reflected, they'd like to see specific
                time
                information. We learned these results from scenario 1, 2, 3 and 6.</p>


              <h4>LG users expect a top-tier navigation experience with multiple routes and the information required to
                choose between them</h4>
              <p>Through our user enactments, we've learned that our users needed additional information when using the
                navigation in LG. Detailed information about where they're heading, another point of interests, and
                fitness
                related information (steps, calories burned, etc) were common health metrics that several users had
                talked
                about. We learned these results from scenario 1, 2, 3, 4, 6 and, 7.</p>


              <h4>LG design contains a constant tension between automation and control</h4>
              <p>One confounding area that we found was based on giving control to the user and how they interacted with
                the
                system. Although they wanted the system to do most of the automation in terms of navigation and
                suggesting
                exercises, users still wanted the ability to choose between different routes and even decline exercises
                they
                didn't want to do. We learned these results from scenario 1, 3, 4, 5, 6 and, 7.</p>

              <h4>Users want to control LG primarily via voice, but with a gesture for fine control actions</h4>
              <p>During our ideation phase, we had believed that users would interact with our system primarily with the
                buttons on the sides of the device. We have come to realize that this was false and it was more natural
                for
                our users to use voice. Although most used voice, some users preferred the use of gestures for finer
                control
                selections. One example of a “finer control” situation was the stop selection screen from Scenario 1,
                where
                users wanted to use gestures to control which stops were selected. We learned these results from
                scenario 1,
                2, 3, 5 and, 7.</p>

              <h4>The more complex LG actions require information step-by-step instructions</h4>
              <p>After completing the user enactments, we had learned that the users were confused about what certain
                terms
                or actions were supposed to convey. Users had to conduct trial and error to learn what the terms or
                actions
                were supposed to do. This issue showed up in several areas, but most noticeable in our mockups of the
                first-time setup. We will have to take concentrate on onboarding in particular, as this concept is
                emerging
                technology that many users won’t have a basis to understand. We learned these results from scenario 2,
                3, 5,
                6 and, 7.</p>

              <h4>The UI Design of LG should be targeted to support the active walking/running experience</h4>
              <p>The user interface failed to convey the necessary information while the users were active. This problem
                was
                proportional to the amount of text available on the screen. This was also evident in scenarios where
                users
                were not active but were still overwhelmed by the amount of information. Although this was evident
                throughout all of our scenarios, it was especially key during the scenarios 1, 2, 3, 6 and, 7.</p>

              <h4>LG users expect to receive informative, motivating feedback on the efficacy of their exercise</h4>
              <p>We also learned that users enjoyed and expected positive feedback at the end of a route or an exercise.
                Many users voiced their thoughts that this would motivate them to continue using LG. They also enjoyed
                seeing the health-related information that showed the impact of choosing a longer route or spontaneous
                exercise. These two points were mainly reflected in a summary screen in scenarios, 1,2 and, 6.</p>

              <h4>The UI has to reflect on the specific actions a user can take</h4>
              <p>Many users were confused on how we presented information on in the AR system. Therefore, they failed to
                identify the interactions and conceptualization of what to do in certain areas. This was reflected in
                scenario 1, 3, 4, 5, 6 and, 7.</p>

              <h4>Importance of Safety</h4>
              <p>Safety was a big consideration after the fact that users claimed that they may be afraid to use this
                device
                in public or at night. We had not predicted this consideration but since two users brought this to our
                attention, we thought it would be best to highlight it. This was reflected in scenarios 2 and 3.</p>


              <h3>Post-User-Enactment Ideation and Selection</h2>
                <p>
                  Based on the research findings, we felt like we had enough information to ideate and further refine
                  our
                  design concept. To guide our refining, we used the following core principles, which were derived
                  directly
                  from the study findings we describe in the section above:

                  <ol>
                    <li>Prioritizing individual usage</li>
                    <li>User perceptions about public exercises</li>
                    <li>Better user control</li>
                    <li>Respect user privacy concern</li>
                    <li>Supportive guidance</li>
                    <li>Non-Distracting User Interface</li>
                  </ol>
                  <br>


                  Below are the design decisions we made regarding how LG could better serve users through recommending
                  and
                  fitting
                  exercises into their daily lives.</p>

                <h4>1. Focus on individual usage scenarios</h4>
                <p>According to our research, users feel uncomfortable with involving their friends in LG activities.
                  Most
                  of them would not use this feature in social settings. Therefore, we decided to focus on individual
                  usage
                  instead of involving other users into activities. We won’t build in any activity features where
                  multiple
                  people are using LG routes together.</p>

                <h4>2. Recommending walking and running as primary exercises, climbing as secondary</h4>
                <p>We found that users feel awkward doing some exercises in public such as stretching, but walking and
                  running are what most users are comfortable with, and climbing is what some users really enjoy.
                  Therefore,
                  we decided to focus on these three opportunity areas.</p>

                <h4>3. Display information visually and verbally, and support user input with voice and gesture control
                </h4>
                <p>Users worked with voice and gesture controls extensively in our study, and text is an important
                  assistant
                  for users especially in a noisy environment. However, the amount of text we used in the study was too
                  much
                  for users to read and was placed in such a manner as to impede the users sight. Therefore, we will
                  keep
                  the voice interaction, reduce the text to a minimum amount, and place text on side part of the LG view
                  to
                  ensure users can clearly see through LG glasses.</p>

                <h4>4. Provide multiple choices</h4>
                <p>Overall, users prefer having moderate levels of choice, particularly in the realms of route selection
                  and
                  user profile preference. When LG detects an opportunity area for users, it will provide a multiple
                  exercise recommendations for users to choose. When users are moving from one spot to another, LG can
                  suggest multiple routes with information such as choice in time taken, intensity of exercise, and
                  distance.</p>

                <h4>5. Only track exercise relevant data</h4>
                <p>Users had privacy concerns and did not like to have LG access their friend lists, contact information
                  and
                  any other personal background information, so we will only track user calories and exercises for user
                  reference. We won’t track information such as home locations, social contacts, time spent at a
                  location,
                  or other details that could be used to infer personal information the user isn’t actively sharing.</p>

                <h4>6. Provide calories, time, and health relevant information</h4>
                <p>Based on user feedback, calories burned was the most important health information to be tracked. Time
                  is
                  also important because they do not want to be late due to doing extra exercises on their way, and they
                  prefer to see how much time will take and how much time left for them to get to their destination. In
                  addition,. Our subjects mentioned some existing products and services such as Fitbit and MyFitnessPal
                  as
                  comparator products that show good amounts of health information. We will relook at these comparator
                  products, and use their health visualizations as a template for our own. We will also look at adding a
                  personal goal structure to allow users to specify goals that they can measure their health information
                  against.</p>

                <h4>7. Provide enough guidance for on boarding and data usage</h4>
                <p>LG represents powerful, emerging technology, and a correct, informative onboarding sequence seems
                  necessary to introduce our users to the product, based on our study results. We will need to provide
                  an
                  onboarding walkthrough that explains basic AR concepts, as well as the primary features of the app.
                  Our
                  first-time setup phone experience will need to be particularly informative, so that users know what
                  the
                  different setting are, and how they affect the glasses experience. In addition, we will articulate how
                  the
                  collected data will be used, so users will be informed and will be more comfortable with providing the
                  data.</p>

                <h4>8. Non-Distracting User Interface</h4>
                <p>UI design should be optimized to support the visual experience of walking/running. UI design should
                  be
                  non-intrusive and minimalistic, allowing the user to maintain situational awareness of the real-world
                  environment.</p>

                <h2>LG Prototype - Bringing the Future of Exercise To You Today! (sort-of)</h2>
                <p>
                  Now that we had our overall design concept is place, complete with user input on specific feature
                  areas,
                  we could
                  realistically begin building a prototype to fully demonstrate LG as an exercise/time management tool.
                  We
                  began by defining out a System Proposal for what specific funtionality
                  we would incorporate, as well as a Demo Proposal to determine the hardware/software components to be
                  used.
                </p>

                <h3>System Proposal</h3>
                <p>Our final system proposal was a time-management system designed to integrate cardiovascular exercise
                  into
                  the daily non-exercise routine of a university student. As the student goes about their daily exercise
                  activity, the system will scan their calendar system and compare it with location data to generate
                  ad-hoc
                  walking/running routines that still fit with the students schedule. These system consists of the
                  following
                  sub-components:

                  <ul>
                    <li>A set of AR glasses that control the primary walking/running experience. These glasses overlay
                      the
                      users vision, and provide Google Maps style walking/running guidance. At the beginning of their
                      travel
                      routine, users put on and activate their glasses. The AR glasses then calculate possible
                      walking/running routes based on a variety of user-specific information, and then provides the user
                      a
                      choice of routes. The primary categorization metric for the route selection will be time before
                      the
                      next event on the user's schedule, but the app will provide additional selection criteria, such as
                      route distance, calories burned, points of interest along the route, and other factors. Once a
                      route
                      has been selected, the glasses provide step-by-step navigation, scanning and updating the
                      estimated
                      time of arrival as needed based on real-world conditions. Once the route has been completed, the
                      glasses will provide a summary screen showing how the exercise fits into the users overall
                      exercise
                      experience.</li>
                    <li>
                      A companion smartphone app that lets the user view and control functionality that would be awkward
                      to
                      deal with while walking around. The companion app includes the following control features:
                      <ul>
                        <li>The ability to enter in personal metrics (height, weight, etc) that the glasses will use to
                          generate routes.</li>
                        <li>The ability to select what calendars the glasses can see and make assumptions on.</li>
                        <li>The ability to enter in personal health goals, which the glasses will also use as input into
                          its
                          routing algorithm.</li>
                      </ul>
                    </li>
                  </ul>
                </p>

                <h3>Demo Construction</h3>
                <p>

                  As the resident AR expert on the team, the responsibility for the design and implementation of the
                  initial
                  prototype. As we had 3 weeks to develop a
                  prototype. My inital plan was to create a "Wizard of Oz" prototype that would simulate the actual
                  intended
                  experience. The primary aspect we "Wizarded"
                  was the environment scanning functionality of LG, as that required computer vision and context
                  awareness
                  technology that we believed did not yet exist.
                  Instead, my plan was to use an simpler AR app with appropriately designed UI components to give
                  the impression of a working app. The app would use prepositioned paper AR markers placed in a physical
                  environment.
                  Upon detecting this markers, the AR app would trigger UI changes that would seems as though the app
                  had
                  scanned the environment.
                  <br> <br>
                  I built the following hardware/software component list based on the hardware we had access to, and
                  software I had at least some familiarity with:

                  <br><br>

                  <strong>Hardware: </strong>

                  <ul>
                    <li>Samsung Gear VR :The Samsung Gear VR is a VR headset requires a compatible Samsung phone to be
                      inserted into
                      it as the display/processing unit. The phone's back camera is kept clear while inserted, so it was
                      my
                      hope to be able to access the camera and use it to
                      create an AR experience WITHIN the Gear VR's default VR experience.
                    </li>
                    <li>Samsung Galaxy S7 smartphone: I owned a phone compatible with the Gear VR. </li>
                    <li>Image Marker printouts: As discussed before, my plan was to use Image Targets (small paper
                      cutouts
                      with distinctive images
                      that AR apps can easily recognize) to simulate the "scanning" component of the LG concept. We
                      would
                      pre-position Image Targets along our demonstration
                      route, and scanning the Image Marker would trigger the corresponding content. As an example, if we
                      wanted to simulate a stair-exercise, we would pre-position an AR marker on the stairs,
                      and tie the appropriate UI visuals to appear when the Image Marker was scanned.
                    </li>
                  </ul>


                  <strong>Software: </strong> :

                  <ul>

                    <li>AFrame AR/VR framework: We would build the core application in AFrame. AFrame is a web-based
                      framework that
                      allows for VR/AR experience to run right in a webpage. I was already familiar with the basics of
                      AFrame, having worked
                      with it on my own over the previous summer. One advantage of this approach is that the
                      applications
                      developed
                      will run on any device that has a compatible browser (Firefox or Chromium), and access to a
                      camera.
                      This would
                      give us the ability to record experiences on both the AR glasses and with a laptop, allowing us to
                      piece
                      together an overall video experience using a variety of platforms. I already had an appropriate
                      AFrame-enabled website active, and we simply added a new webpage
                      to the website for our LG prototype app.</li>
                    <li>Oculus VR App: The Gear VR requires the use of a special app that automatically activates when
                      the
                      phone
                      is inserted into the Gear VR headset. My plan was to run the AFrame experience on a web browser
                      pulled
                      up within the default Oculus environment.</li>

                    <li>Youtube Gaming: We wanted to capture the visual experience of our AR app as prospective users
                      experience it. Our plan was to use the Youtube Gaming app (a smartphone app designed to
                      stream/record
                      mobile games) to capture the screen and
                      microphone
                      output of the phone as it was placed in the Gear VR. This would allow us to demo the experience at
                      scale, as several presenters could watch one person actually use the prototype.</li>
                  </ul>

                  <h3>Demo Iteration 1 - Technical Concept Testing</h3>
                  <p>
                    The primary area of unknowns were the use of AFrame within the Gear VR environment, and streaming
                    the
                    resulting app to multiple presentees. As my teammate worked on developing the visual
                    design for the UI components, I began development on a basic proof-of-concept application to test
                    these
                    unknowns. The proof-of-concept was simple, with a single webpage, 2 UI elements that could be
                    switched
                    out with a push
                    of a button. The proof-of-concept revealed three development issues that necessiated a need to
                    re-think
                    the development approach:
                    <ol>
                      <li>The Gear VR app, while capable of running AFrame in a web browser, prioritized the VR
                        environment
                        the web browser was in. As discussed before, the Gear VR app runs by default as a virtual
                        reality
                        environment, and the web browser
                        is launched as single 2-D window WITHIN that virutal environment. This meant that when the user
                        turned their head, instead of panning the real-world view in the AFrame browser experience, the
                        user's view would instead pan around the virtual environment the browser was in, looking away
                        from
                        the browser window itself. This issue rendered the Gear VR approach unusable.
                      </li>
                      <li>The Gear VR's interaction system was not user-friendly. I owned a first-generation Gear VR
                        system,
                        which did not include a seperate hand-held control unit. Instead, the unit
                        has a 5-point touchpad located on the right side of the headset itself. The user must interact
                        with
                        the touchpad by feel, reaching up to the side of their head. While I was able to get the AFrame
                        website to respond to taps on this touchpad, we found the experience physically awkward, and
                        decided
                        that we didn't want this to be a part of our prototype.
                      </li>
                      <li>We found that the performance strain of using the Gear VR app, the AFrame functionality, and
                        the
                        Youtube streaming service together was more than
                        my phone could handle. Streaming performance was exceptionally bad, and battery life was
                        extremely
                        limited under these conditions.
                      </li>
                    </ol>
                  </p>

                  <h3>Demo Iteration 2 - Rehearsed Route and Full Demo Development</h3>

                  <p>Based on these issues, as well as the fact that we had a limited window with which to develop the
                    overall prototype,
                    I pivoted the design to a more simple Wizard-of-Oz approach. We would run the experience directly on
                    a
                    phone, without a headset. This
                    would allow us to run the demo on a newer Google Pixel phone, which was capable of running AFrame
                    and
                    Youtube Gaming streaming with good performance.
                    I also changed the format of the demo experience to match the new platform. We would be running our
                    live
                    demo purely as a livestream, with one of our team members
                    performing the demo on the phone. We would have our team member walk a pre-determined route in the
                    hallways around the demo room, holding the phone in such a manner as to give the impression of a
                    first-person POV AR glasses experience when viewed on the livestream.
                    The team member would then walk the route, provided audio commentary as they did so. <br> <br>

                    I decided on one more simplification, given the new changes. I decided to not involve the Image
                    Markers
                    at all, as those, while still relative reliable,
                    can be subject to computer vision recognition issues. Instead, I decided on a "Powerpoint" approach.
                    Each discreet portion within the route would have its own set of pre-built UI elements, with only
                    the
                    first
                    section's UI being visible initially. As the demonstrator walked the route, he would tap on the
                    phone
                    screen upon entering each new route sections. This tap (invisible to the live streamers) would make
                    the
                    previous section's UI
                    invisible, and make the current section visible. Similar to Powerpoint slides, the demonstrator
                    would
                    simply iterate through each UI screen in turn. <br><br>

                    To add a small bit of artistry to the whole experience, we built the route to be circular, starting
                    just
                    outside the demo room, and ending with the demonstrator walking into the demo room. We also chose
                    not
                    to inform our demo audience that the demostration was in fact, live, which elicited an enthusiatic
                    reaction from our demonstration viewers when they realized what had just happened.

                    Once this new plan had been devised, the development was relatively straightforward. My team mates
                    created the Photoshop UI mockups, and I then
                    pre-positioned them on the AFrame webpage. I then built a Javascript framework to switch what UI
                    element
                    was visble, changing based on taps on the phone screen. All together,
                    12 separate "screens" were created to show the various capabilities of LG. <br><br>

                    The demo website remains live at <a
                      href="https://halafaireet.github.io/lifeasagym/">https://halafaireet.github.io/lifeasagym/</a>
                    <br><br>
                    The code I built is available at <a
                      href="https://github.com/HAlafaireet/lifeasagym/blob/master/index.html">https://github.com/HAlafaireet/lifeasagym/blob/master/index.html</a>
                    <br><br>
                    Below is a demonstration video showing how this demo approach worked in practice.

                    <div class="embed-responsive embed-responsive-16by9">
                      <iframe width="560" height="315" src="https://www.youtube.com/embed/OIUaA9tzm0I" frameborder="0"
                        allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                        allowfullscreen></iframe>
                    </div>

                  </p>

                  <h2>Final Steps</h2>
                  <p>
                    Once we had our live demo in place, we built the rest of the final presentation materials. This
                    included
                    a pitch deck that also
                    summarized our primary research findings, as well as a more polished final presentation video that
                    relayed a higher-level narrative scenario. We reused our UI assets
                    from our live demonstration into this final video (which is the video available for view in the
                    Project
                    Overview section of the final document).

                    <br><br>
                    Our demonstration attendees reacted quite positively to our pitch deck and live demo. Despite the
                    futuristic nature of our
                    design concept, we were able to successfully deliver a compelling vision of the exercise
                    time-management
                    support Life As A Gym could provide University student in their busy lives.
                  </p>

        </div>
      </div>
    </div>

  </main>

  <footer class="footer">
    <div>
      <p>&copy; 2019 Sainy Alafaireet</p>
    </div>
  </footer>

</body>

</html>